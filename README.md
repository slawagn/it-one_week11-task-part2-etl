В файле `spark-etl.py` содержится простой код, перекладывающий данные из одной
БД в другую для демонстрации.

В `.gitlab-ci.yml` определён простой CI-процесс с запуском скрипта `test/tesh.sh`
если коммит отправлен в ветку `test`.

Сам тест выглядит слдующим образом:

- Поднимаются три контейнера: два с БД и третий со Spark;

- В контейнер с src-БД изначально маунтится `seed.sql`, наполняющий таблицы;

- В контейнере со Spark запускается `spark-etl.py`;

- В контейнере с dest-БД запускается `test.sql`, проверяющий содержимое БД
на соответствие ожиданиям.

У данного подхода есть недостаток: тест падает на первом же несооответствии,
а не выводит все ошибки. При необходимости можно выделить время и реализовать
тесты по-другому (например, с помощью Python-скрипта, а не sql-файла),
но зато продемонстрированный вариант очень прост и тесты очень легко
запустить в том числе и локально.
